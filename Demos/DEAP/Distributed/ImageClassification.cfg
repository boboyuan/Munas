[general]

BlockArchitecture = ResNetBlockArchitecture
ClassCount = 10

Verbose = False
Multithreaded = False
Distributed = True
DatasetModule = Demos.Datasets.Cifar10
# Set the number of threads to be used, 0 does not set a limit, -1 sets the number to the number presented
# by the CPU to the OS
ThreadCount = 1
# Please note that if using the GPU you should run the program using a single thread, multi-threaded training usually
# leads to GPU memory issues as GPU memory management struggles to allocate memory between threads.
GPU = True
Log = True

[evolution]

VerboseMutation = False
MutationProbability = 0.5
CrossoverProbability = 0.1
#number of times an architecture is attempted to be mutated should it mutate into an invalid architecture
MutationAttempts = 10
RetrainEveryGeneration = False

# Mutation can happen in one of two ways, either there is an EQUAL probability for every block in the hierarchy or
# via the given (or default) self-mutation PROBABILITY value, eg. 0.5 self-mutation means that each block has a 50% chance to
# mutate itself. As such the value can be used to shift the focus on mutating to either the top or bottom of the
# hierarchy, eg. 0.5 means the second block layer has a 50% chance while the third has a 25% chance, similarly a 0.9
# value gives the second block layer a 9% chance while the third layer has a 81% chance, assuming a hierarchy of depth
# 3.
; MutationMethod = EQUAL
MutationMethod = PROBABILITY
SelfMutationProbability = 0.8

# Certain search processes use a mutation probability that causes mutation to happen at increasingly deeper levels during
# the search progress, eg. simulated annealing. Thus, one can set the change that should be applied to the mutation
# probability with each generation, either positive (shifting mutation lower) or negative (shifting mutation higher).
VariableMutationGenerationalChange = 0.03

PopulationSize = 120
GenerationCount = 16

[output]

FigureTitle = Goal Attainment
SaveIndividuals = True
OutputPrefix = ImageClassification

# How often should the generations be visualized
GenerationGap = 1

# Saving individuals can be done, EVERY generation or at a specified INTERVAL, the final generation is always saved
GenerationSave = EVERY
GenerationSaveInterval = 1

[goals]

# In a goal vector array the normalization vector or goal vector can be varied to create the array, if set to True
# the goal vector will be varied, otherwise the normalization vector will be varied
# The goal vector is made up of two values, the parameter count goal and the accuracy goal. The goal vector can
# be created such that either value can be varied, or both. If a value is to be a fixed value then the goal vector
# entries start and stop value should be the same.

# Either the goal vector can be varied or the noramlization vector, VariableGoal = true will vary the goal vector
# while setting it to false will vary the normalization vector.
VariableGoal = True

### Varied Goal vector
GoalParamVectorStart =1000000
GoalParamVectorEnd =400000
GoalAccVectorStart = 100
GoalAccVectorEnd = 100

# To set incremental steps set the steps to 0
GoalVectorSteps = 5
NormalizationVector = (1000, 1)

### Varied Normalization Vector

; VariableGoal = False

NormalizationParamVectorStart =100
NormalizationParamVectorEnd =500
NormalizationAccVectorStart = 0.1
NormalizationAccVectorEnd = 0.15

NormalizationVectorSteps = 6
GoalVector = (40000, 80)

[filter]

FilterFunction = MinMaxArray
FilterFunctionModule = TensorNAS.FilterFunctions.MinMax

# The multi-objective optimization requires that every objective has a weight that signifies if it should be
# minimized or maximized, as such the weights can be set to be all 'maximized', all 'minimize' or manually set,
# eg. -1, 1, -1

Weights = minimize

[tensorflow]

TrainingSampleSize = 0
TestSampleSize = 0
Optimizer = adam
Loss = tf.keras.metrics.categorical_crossentropy
Metrics = accuracy
EarlyStopper = True
BatchSize = 0
Epochs = 500
QuantizationAware = False

# learning rate scheduler
[lrscheduler]
UseLRScheduler = True
InitialLearningRate = 0.001
DecayPerEpoch = 0.99


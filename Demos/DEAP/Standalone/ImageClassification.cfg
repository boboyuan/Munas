[general]

BlockArchitecture = ResNetBlockArchitecture
ClassCount = 10

Verbose = True
Multithreaded = False
Distributed = False
DatasetModule = Demos.Datasets.Cifar10
# If you are running a non-distributed demo but would like the dataset stored local to the demo code
LocalDataset = True
GenBlockArchitecture = Demos.gen_classification_ba
# Set the number of threads to be used, 0 does not set a limit, -1 sets the number to the number presented
# by the CPU to the OS
ThreadCount = 1
# Please note that if using the GPU you should run the program using a single thread, multi-threaded training usually
# leads to GPU memory issues as GPU memory management struggles to allocate memory between threads.
GPU = True
Log = True

[evolution]

VerboseMutation = False
MutationProbability = 0.5
CrossoverProbability = 0.1
#number of times an architecture is attempted to be mutated should it mutate into an invalid architecture
MutationAttempts = 10
RetrainEveryGeneration = False
# Alpha value used for mutation function reinforcement learning
Alpha = 0.2

# Mutation can happen in one of two ways, either there is an EQUAL probability for every block in the hierarchy or
# via the given (or default) self-mutation PROBABILITY value, eg. 0.5 self-mutation means that each block has a 50% chance to
# mutate itself. As such the value can be used to shift the focus on mutating to either the top or bottom of the
# hierarchy, eg. 0.5 means the second block layer has a 50% chance while the third has a 25% chance, similarly a 0.9
# value gives the second block layer a 9% chance while the third layer has a 81% chance, assuming a hierarchy of depth
# 3.
; MutationMethod = EQUAL
MutationMethod = PROBABILITY
SelfMutationProbability = 0.75

# Certain search processes use a mutation probability that causes mutation to happen at increasingly deeper levels during
# the search progress, eg. simulated annealing. Thus, one can set the change that should be applied to the mutation
# probability with each generation, either positive (shifting mutation lower) or negative (shifting mutation higher).
VariableMutationGenerationalChange = 0.03

PopulationSize = 40
GenerationCount = 16

[output]

FigureTitle = Goal Attainment
SaveIndividuals = True
OutputPrefix = ImageClassification

# How often should the generations be visualized
GenerationGap = 1

# Saving individuals can be done, EVERY generation or at a specified INTERVAL, the final generation is always saved
GenerationSave = EVERY
GenerationSaveInterval = 1

[goals]

# In a goal vector array the normalization vector or goal vector can be varied to create the array, if set to True
# the goal vector will be varied, otherwise the normalization vector will be varied
# The goal vector is made up of two values, the parameter count goal and the accuracy goal. The goal vector can
# be created such that either value can be varied, or both. If a value is to be a fixed value then the goal vector
# entries start and stop value should be the same.

# Either the goal vector can be varied or the noramlization vector, VariableGoal = true will vary the goal vector
# while setting it to false will vary the normalization vector.
VariableGoal = True

### Varied Goal vector
GoalParamVectorStart = 78666
GoalParamVectorEnd = 78666
GoalAccVectorStart = 61
GoalAccVectorEnd = 61

# To set incremental steps set the steps to 0
GoalVectorSteps = 1
NormalizationVector = (1000, 1)

### Varied Normalization Vector

; VariableGoal = False

NormalizationParamVectorStart = 100
NormalizationParamVectorEnd = 500
NormalizationAccVectorStart = 0.1
NormalizationAccVectorEnd = 0.15

NormalizationVectorSteps = 6
GoalVector = (40000, 80)

[filter]

FilterFunction = MinMaxArray
FilterFunctionModule = TensorNAS.FilterFunctions.MinMax

# The multi-objective optimization requires that every objective has a weight that signifies if it should be
# minimized or maximized, as such the weights can be set to be all 'maximized', all 'minimize' or manually set,
# eg. -1, 1, -1

Weights = minimize

[tensorflow]

# -1 meaning to take the absolute length of the input training & test data
TrainingSampleSize = -1
TestSampleSize = -1
ValidationSplit = 0.1
Optimizer = adam
Loss = categorical_crossentropy
Metrics = accuracy
EarlyStopper = True
# If early stopping is enabled one must specify how much patience one wants
Patience=3
StopperMonitor=accuracy
StopperMinDelta=0.005
StopperMode=max

# For 6GB GPU
BatchSize = 512
# For 24GB GPU
# BatchSize = 1028
TestBatchSize = 32
Epochs = 500
QuantizationAware = False

# learning rate scheduler
[lrscheduler]
UseLRScheduler = True
LRScheduler = lrscheduler_decay
InitialLearningRate = 0.001
DecayPerEpoch = 0.99

[image data generator]

RotationRange = 15
WidthShiftRange = 0.1
HeightShiftRange = 0.1
HorizontalFlip = True
ValidationSplit = 0.2
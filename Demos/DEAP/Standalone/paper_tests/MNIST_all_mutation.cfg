[general]

BlockArchitecture = MNISTBlockArchitecture
ClassCount = 10

Verbose = True
Multithreaded = False
Distributed = False
DatasetModule = Demos.Datasets.MNIST
GenBlockArchitecture = Demos.gen_classification_ba
# Set the number of threads to be used, 0 does not set a limit, -1 sets the number to the number presented
# by the CPU to the OS
ThreadCount = 1
# Please note that if using the GPU you should run the program using a single thread, multi-threaded training usually
# leads to GPU memory issues as GPU memory management struggles to allocate memory between threads.
GPU = True
Log = True

[evolution]

VerboseMutation = False
MutationProbability = 0.5
CrossoverProbability = 0.1
# Number of times an architecture is attempted to be mutated should it mutate into an invalid architecture
MutationAttempts = 10
RetrainEveryGeneration = False
UseReinforcementLearningMutation = True
# Alpha value used for mutation function reinforcement learning
Alpha = 0.2

# Mutation can happen in one of two ways, either there is an EQUAL probability for every block in the hierarchy or
# via the given (or default) self-mutation PROBABILITY value, eg. 0.5 self-mutation means that each block has a 50% chance to
# mutate itself. As such the value can be used to shift the focus on mutating to either the top or bottom of the
# hierarchy, eg. 0.5 means the second block layer has a 50% chance while the third has a 25% chance, similarly a 0.9
# value gives the second block layer a 9% chance while the third layer has a 81% chance, assuming a hierarchy of depth
# 3.
# To have ALL mutation methods in the entire block architecture as equally probable, one can also specity ALL.
; MutationMethod = EQUAL
;MutationMethod = PROBABILITY
MutationMethod = ALL
SelfMutationProbability = 0.75

# Certain search processes use a mutation probability that causes mutation to happen at increasingly deeper levels during
# the search progress, eg. simulated annealing. Thus, one can set the change that should be applied to the mutation
# probability with each generation, either positive (shifting mutation lower) or negative (shifting mutation higher).
# PopulationSize= 40
# PopulationSize factor of 4
# GenerationCount = 16
# GenerationCount min 3
VariableMutationGenerationalChange = 0.03

PopulationSize = 8                  
GenerationCount = 5

[output]

FigureTitle = Goal Attainment
SaveIndividuals = True
OutputPrefix = MNIST_all_mutation

# How often should the generations be visualized
GenerationGap = 1

# Saving individuals can be done, EVERY generation or at a specified INTERVAL, the final generation is always saved
GenerationSave = EVERY
GenerationSaveInterval = 1

[goals]

# In a goal vector array the normalization vector or goal vector can be varied to create the array, if set to True
# the goal vector will be varied, otherwise the normalization vector will be varied
# The goal vector is made up of two values, the parameter count goal and the accuracy goal. The goal vector can
# be created such that either value can be varied, or both. If a value is to be a fixed value then the goal vector
# entries start and stop value should be the same.

# Either the goal vector can be varied or the noramlization vector, VariableGoal = True will vary the goal vector
# while setting it to false will vary the normalization vector.
VariableGoal = True

### Varied Goal vector
GoalParamVectorStart = 3000
GoalParamVectorEnd = 3000
GoalAccVectorStart = 90
GoalAccVectorEnd = 90
GoalFlopsVectorStart=10000000
GoalFlopsVectorEnd= 10000000

# To set incremental steps set the steps to 0
GoalVectorSteps = 1
NormalizationVector = (500, 1, 400000)

### Varied Normalization Vector

; VariableGoal = False

NormalizationParamVectorStart =100
NormalizationParamVectorEnd =500
NormalizationAccVectorStart = 0.1
NormalizationAccVectorEnd = 0.15

NormalizationVectorSteps = 6
GoalVector = (40000, 80)

[filter]

FilterFunction = MinMaxArray
FilterFunctionModule = TensorNAS.FilterFunctions.MinMax

# The multi-objective optimization requires that every objective has a weight that signifies if it should be
# minimized or maximized, as such the weights can be set to be all 'maximized', all 'minimize' or manually set,
# eg. -1, 1, -1

Weights = minimize

[tensorflow]

# -1 meaning to take the absolute length of the input training & test data
TrainingSampleSize = -1
TestSampleSize = -1
ValidationSplit = 0.1
Optimizer = adam
Loss = sparse_categorical_crossentropy
Metrics = accuracy
EarlyStopper = True
Patience=4
StopperMonitor=val_accuracy
StopperMinDelta=0.005
StopperMode=max

# 6GB GPU
BatchSize = 256
# For 24GB GPU
# BatchSize = 10000
TestBatchSize = 32
Epochs = 200
QuantizationAware = False